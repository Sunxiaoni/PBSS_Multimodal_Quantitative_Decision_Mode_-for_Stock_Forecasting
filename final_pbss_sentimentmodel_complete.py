# -*- coding: utf-8 -*-
"""final_PBSS_SentimentModel_complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CI-kZOvv5qRV-ocFNuD_TQyjbit1OIOm
"""

!pip -q install serpapi newspaper3k transformers torch nltk --upgrade

!pip install lxml_html_clean

# æŒ‚è½½ Google Driveï¼ˆåªéœ€è¿è¡Œä¸€æ¬¡ï¼‰
from google.colab import drive
drive.mount('/content/drive')

# ================== Full Sparse Sentiment Pipeline (REAL publish date + TZ fix) ==================
# è¿‘7å¤©æ–°é—» â†’ é¡µé¢å‘å¸ƒæ—¶é—´(meta/JSON-LD/URL) + ç›¸å¯¹æ—¶é—´è§£æ â†’ FinBERT+LM æ‰“åˆ† â†’ å»é‡ â†’ ç¨€ç–èšåˆ(T / T+1) â†’ ä¿å­˜
# è¯´æ˜ï¼šä¸è¡¥é½ã€ä¸å‰å¡«ï¼›å“ªå¤©æœ‰æ–°é—»å“ªå¤©ä¸€è¡Œã€‚å¸¦ ET/CDT/PDT ç­‰ç¼©å†™çš„æ—¶é—´ä¼šè¢«è§„èŒƒåŒ–åˆ° UTCã€‚

# ---------- 0) ä¾èµ–å®‰è£… & Drive ----------
import sys, subprocess, importlib
def _ensure(pkgs):
    for p in pkgs:
        try: importlib.import_module(p)
        except ImportError: subprocess.check_call([sys.executable, "-m", "pip", "install", p])
_ensure(["requests","newspaper3k","transformers","torch","pandas","numpy"])

#from google.colab import drive
#drive.mount('/content/drive')

# ---------- 1) Imports & é…ç½® ----------
import os, re, html, time, requests, numpy as np, pandas as pd, torch
from datetime import datetime, timezone, timedelta
from urllib.parse import urlparse
from pandas.tseries.offsets import BDay
from zoneinfo import ZoneInfo
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# SerpAPI
SERPAPI_ENDPOINT = "https://serpapi.com/search.json"
DEFAULT_LOCALE   = "United States"
RESULTS_PER_PAGE = 10
MAX_PAGES_PER_TICKER = 5
SLEEP_BETWEEN_CALLS  = 0.7
SERPAPI_HL, SERPAPI_GL = "en", "us"

# ä¿å­˜
SAVE_DIR    = "/content/drive/MyDrive"
SAVE_PREFIX = "sentiment"

# æŸ¥è¯¢æ˜ å°„
TICKER_QUERY_MAP = {
    "AAPL": "Apple news OR Apple Inc",
    "MSFT": "Microsoft news OR Windows",
    "GOOGL": "Google OR Alphabet news",
    "META": "Meta OR Facebook news",
    "TSLA": "Tesla news OR Elon Musk",
    "NVDA": "NVIDIA news OR GPU",
    "NFLX": "Netflix news",
    "AMZN": "Amazon news",
    "AMD":  "AMD news OR Advanced Micro Devices",
    "AVGO": "Broadcom news",
}

# ---------- 2) æ—¶é—´è§£æï¼ˆç›¸å¯¹æ—¶é—´ + ç¼©å†™æ—¶åŒº â†’ UTCï¼‰ ----------
_REL_MAP = {
    "min": "minute","mins":"minute","minute":"minute","minutes":"minute",
    "hour":"hour","hours":"hour","hr":"hour","hrs":"hour",
    "day":"day","days":"day",
    "week":"week","weeks":"week",
}
def _parse_relative_ago(s: str):
    if not isinstance(s, str): return None
    s = s.strip().lower()
    m = re.search(r"(\d+)\s+([a-z]+)\s+ago", s)
    if not m: return None
    n = int(m.group(1)); unit = _REL_MAP.get(m.group(2))
    if not unit: return None
    now = datetime.now(timezone.utc)
    if unit == "minute": return now - timedelta(minutes=n)
    if unit == "hour":   return now - timedelta(hours=n)
    if unit == "day":    return now - timedelta(days=n)
    if unit == "week":   return now - timedelta(weeks=n)
    return None

# ç¼©å†™æ—¶åŒº â†’ IANA
_TZ_ABBR_MAP = {
    "ET":"America/New_York","EST":"America/New_York","EDT":"America/New_York",
    "CT":"America/Chicago","CST":"America/Chicago","CDT":"America/Chicago",
    "MT":"America/Denver","MST":"America/Denver","MDT":"America/Denver",
    "PT":"America/Los_Angeles","PST":"America/Los_Angeles","PDT":"America/Los_Angeles",
}
def _normalize_datetime_with_tz_abbr(s: str):
    """æ”¯æŒ '2025-10-14T03:18ET' / 'Tue Oct 14, 1:54AM CDT' â†’ UTC Timestamp"""
    if not isinstance(s, str) or not s.strip(): return None
    s = s.strip()
    m = re.search(r'\b([A-Z]{2,3}T?|[ECMP][SD]?T)\b', s)
    tz_abbr = m.group(1) if m else None
    tz_name = _TZ_ABBR_MAP.get(tz_abbr) if tz_abbr else None
    s_no_abbr = s if tz_abbr is None else s.replace(tz_abbr, "").strip()
    ts_naive = pd.to_datetime(s_no_abbr, utc=False, errors="coerce")
    if pd.isna(ts_naive):
        s_no_abbr = re.sub(r'\s{2,}', ' ', s_no_abbr).strip()
        ts_naive = pd.to_datetime(s_no_abbr, utc=False, errors="coerce")
    if pd.isna(ts_naive): return None
    if tz_name:
        try:
            ts_local = ts_naive.tz_localize(tz_name, nonexistent='NaT', ambiguous='NaT')
            if pd.isna(ts_local): return None
            return ts_local.tz_convert('UTC')
        except Exception:
            return None
    return pd.to_datetime(ts_naive, utc=True, errors="coerce")

def parse_publish_time(item: dict):
    """SerpAPI å­—æ®µ + ç›¸å¯¹æ—¶é—´ + ç¼©å†™æ—¶åŒº"""
    for key in ("published_at", "date", "time"):
        v = item.get(key)
        if v:
            ts = _normalize_datetime_with_tz_abbr(str(v))
            if ts is None:
                ts = pd.to_datetime(v, utc=True, errors="coerce")
            if pd.notna(ts):
                return ts.to_pydatetime()
            rel = _parse_relative_ago(str(v))
            if rel: return rel
    snip = item.get("snippet") or item.get("summary") or ""
    ts_snip = _normalize_datetime_with_tz_abbr(str(snip))
    if ts_snip is not None:
        return ts_snip.to_pydatetime()
    rel = _parse_relative_ago(str(snip))
    return rel

# ---------- 3) æŠ“å–ï¼ˆSerpAPI + æ­£æ–‡å…œåº• + é¡µé¢å‘å¸ƒæ—¶é—´æå–ï¼‰ ----------
def normalize_query_from_ticker(ticker: str) -> str:
    t = (ticker or "").upper().strip()
    return TICKER_QUERY_MAP.get(t, f"{t} stock news")

def serpapi_news_request(query: str, api_key: str, page: int = 1, locale: str = DEFAULT_LOCALE) -> dict:
    params = {
        "engine": "google_news",
        "q": query,
        "api_key": api_key,
        "num": RESULTS_PER_PAGE,
        "start": (page - 1) * RESULTS_PER_PAGE,
        "location": locale,
        "hl": SERPAPI_HL, "gl": SERPAPI_GL,
        "when": "7d",
    }
    r = requests.get(SERPAPI_ENDPOINT, params=params, timeout=20)
    r.raise_for_status()
    return r.json()

def get_news_from_serpapi(ticker: str, api_key: str):
    query = normalize_query_from_ticker(ticker)
    out = []
    for p in range(1, MAX_PAGES_PER_TICKER + 1):
        js = serpapi_news_request(query, api_key, page=p)
        news = js.get("news_results") or js.get("articles") or []
        if not news: break
        for it in news:
            url = it.get("link") or it.get("url") or ""
            if not url: continue
            src = it.get("source")
            if isinstance(src, dict): src = src.get("name")
            out.append({
                "Ticker": ticker,
                "title_text": it.get("title") or "",
                "url": url,
                "source": src or "",
                "publish_dt": parse_publish_time(it),
            })
        time.sleep(SLEEP_BETWEEN_CALLS)
    return out

def _extract_publish_dt_from_html(html_text: str, url: str):
    """ä» HTML meta / JSON-LD / URL è·¯å¾„æŠ½å–å‘å¸ƒæ—¶é—´ï¼›è¿”å› UTC Timestamp æˆ– None"""
    if not isinstance(html_text, str) or not html_text: html_text = ""
    # meta
    patterns = [
        r'property=["\']article:published_time["\'][^>]+content=["\']([^"\']+)["\']',
        r'name=["\']pubdate["\'][^>]+content=["\']([^"\']+)["\']',
        r'itemprop=["\']datePublished["\'][^>]+content=["\']([^"\']+)["\']',
        r'name=["\']date["\'][^>]+content=["\']([^"\']+)["\']',
        r'property=["\']og:updated_time["\'][^>]+content=["\']([^"\']+)["\']',
    ]
    for pat in patterns:
        m = re.search(pat, html_text, re.I | re.S)
        if m:
            raw = m.group(1).strip()
            ts = _normalize_datetime_with_tz_abbr(raw)
            if ts is None:
                ts = pd.to_datetime(raw, utc=True, errors="coerce")
            if pd.notna(ts): return ts
    # JSON-LD
    m = re.search(r'"datePublished"\s*:\s*"([^"]+)"', html_text, re.I)
    if m:
        raw = m.group(1).strip()
        ts = _normalize_datetime_with_tz_abbr(raw)
        if ts is None:
            ts = pd.to_datetime(raw, utc=True, errors="coerce")
        if pd.notna(ts): return ts
    # URL è·¯å¾„æ—¥æœŸ
    for pat in [r'/(\d{4})/(\d{2})/(\d{2})/', r'/(\d{4})-(\d{2})-(\d{2})(?:/|-)']:
        m = re.search(pat, url)
        if m:
            y, mo, d = m.groups()
            ts = pd.to_datetime(f"{y}-{mo}-{d}", utc=True, errors="coerce")
            if pd.notna(ts): return ts
    return None

def fetch_article_text(url: str):
    """è¿”å› (title, body, publish_dt_page_utc)"""
    try:
        from newspaper import Article
        art = Article(url)
        art.download(); art.parse()
        title = art.title or ""
        body  = art.text or ""
        ts = None
        if getattr(art, "publish_date", None):
            ts = pd.to_datetime(art.publish_date, utc=True, errors="coerce")
        if ts is None or pd.isna(ts):
            ts = _extract_publish_dt_from_html(getattr(art, "html", "") or "", url)
        return title, body, ts
    except Exception:
        try:
            r = requests.get(url, timeout=15); r.raise_for_status()
            t = r.text
            m1 = re.search(r"<title>(.*?)</title>", t, re.I | re.S)
            m2 = re.search(r'<meta[^>]+name=["\']description["\'][^>]+content=["\'](.*?)["\']', t, re.I | re.S)
            title = html.unescape((m1.group(1).strip() if m1 else ""))
            body  = html.unescape((m2.group(1).strip() if m2 else ""))
            ts    = _extract_publish_dt_from_html(t, url)
            return title, body, ts
        except Exception:
            return "", "", None

def discover_and_expand_articles(tickers: list[str], api_key: str) -> pd.DataFrame:
    rows = []
    for tk in tickers:
        base = get_news_from_serpapi(tk, api_key)
        for row in base:
            t2, b2, ts_page = fetch_article_text(row["url"])
            row["title_text"] = row["title_text"] or t2
            row["body_text"]  = b2
            ts_serp = row.get("publish_dt")
            ts = ts_page if ts_page is not None else ts_serp
            if ts is None or pd.isna(pd.to_datetime(ts, errors="coerce")):
                ts = datetime.now(timezone.utc)
            row["publish_dt"] = pd.to_datetime(ts, utc=True, errors="coerce")
            rows.append(row)
    if not rows:
        return pd.DataFrame(columns=["Ticker","publish_dt","title_text","body_text","source","url"])
    df = pd.DataFrame(rows)
    # å¯é€‰ï¼šä»…ä¿ç•™è¿‘7å¤©çœŸå®å‘å¸ƒæ—¶é—´
    # now_utc = datetime.now(timezone.utc)
    # df = df[pd.to_datetime(df["publish_dt"], utc=True) >= (now_utc - pd.Timedelta(days=7))].reset_index(drop=True)
    return df

# ---------- 4) æ‰“åˆ†ï¼ˆFinBERT + ç®€æ˜“ LM ï¼‰ ----------
_FINBERT_MODEL = "yiyanghkust/finbert-tone"
_device   = torch.device("cuda" if torch.cuda.is_available() else "cpu")
_tokenizer= AutoTokenizer.from_pretrained(_FINBERT_MODEL)
_finbert  = AutoModelForSequenceClassification.from_pretrained(_FINBERT_MODEL).to(_device).eval()

@torch.no_grad()
def finbert_scores(texts):
    if not texts: return np.array([]), np.empty((0,3))
    batch_size = 16; all_scores, all_probs = [], []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = _tokenizer(batch, truncation=True, padding=True, max_length=256, return_tensors="pt").to(_device)
        logits = _finbert(**enc).logits
        probs  = torch.softmax(logits, dim=-1).detach().cpu().numpy()  # [neg, neu, pos]
        s = probs[:,2] - probs[:,0]
        all_scores.append(s); all_probs.append(probs)
    return np.concatenate(all_scores), np.concatenate(all_probs)

LM_POS = {"achieve","improve","growth","profit","surge","record","exceed","beat","strong","optimistic","upbeat","bullish"}
LM_NEG = {"decline","downgrade","loss","lawsuit","risk","weak","miss","fraud","bearish","uncertain","bankrupt"}

def lm_score(text: str) -> float:
    if not isinstance(text, str) or not text.strip(): return 0.0
    toks = re.findall(r"[a-zA-Z]+", text.lower())
    pos = sum(t in LM_POS for t in toks); neg = sum(t in LM_NEG for t in toks)
    return 0.0 if pos+neg==0 else (pos - neg) / (pos + neg)

def batch_score(df_articles: pd.DataFrame, w_title=0.3, w_body=0.7, use_lm=True, lm_weight=0.3) -> pd.DataFrame:
    if df_articles.empty: return df_articles.assign(sent_score=[])
    titles = df_articles.get("title_text","").fillna("").tolist()
    bodies = df_articles.get("body_text","").fillna("").tolist()
    fb_title,_ = finbert_scores(titles); fb_body,_ = finbert_scores(bodies)
    fb = w_title*(fb_title if len(fb_title) else 0.0) + w_body*(fb_body if len(fb_body) else 0.0)
    out = df_articles.copy(); out["fb_score"] = fb
    if use_lm:
        out["lm_score"] = [lm_score(f"{t} {b}") for t,b in zip(titles,bodies)]
        s = (1- lm_weight)*out["fb_score"].values + lm_weight*out["lm_score"].values
    else:
        s = out["fb_score"].values
    out["sent_score"] = np.clip(s, -1.0, 1.0)
    return out

# ---------- 5) å»é‡ ----------
def jaccard(a: str, b: str) -> float:
    A = set(re.findall(r"[a-zA-Z0-9]+", (a or "").lower()))
    B = set(re.findall(r"[a-zA-Z0-9]+", (b or "").lower()))
    return 0.0 if not A or not B else len(A & B)/len(A | B)

def dedup_by_title(df: pd.DataFrame, thresh=0.80) -> pd.DataFrame:
    if df.empty: return df
    df = df.sort_values("publish_dt").reset_index(drop=True)
    keep, seen = [], []
    for i, row in df.iterrows():
        t = (row.get("title_text") or row.get("title") or "").strip()
        if not t:
            keep.append(i); continue
        if any(jaccard(t, s) >= thresh for s in seen):
            continue
        seen.append(t); keep.append(i)
    return df.loc[keep].reset_index(drop=True)

# ---------- 6) ç¨€ç–èšåˆï¼ˆT å½“å¤© / T+1 ç”Ÿæ•ˆï¼‰ ----------
def aggregate_daily_T_sparse(df_scored: pd.DataFrame) -> pd.DataFrame:
    if df_scored is None or df_scored.empty:
        return pd.DataFrame(columns=["Ticker","Date","sent_mean_1d","pos_ratio_7d","news_count_1d","sent_mean_3d_ewm","sent_surprise_1d"])
    df = df_scored.copy()
    df["Date"] = pd.to_datetime(df["publish_dt"], errors="coerce").dt.tz_localize(None).dt.normalize()
    df = df.dropna(subset=["Date"])
    daily = (df.groupby(["Ticker","Date"], as_index=False)
               .agg(sent_mean_1d=("sent_score","mean"),
                    news_count_1d=("sent_score","size"),
                    pos_ratio_1d=("sent_score", lambda x: float(np.mean(np.array(x) > 0)))))
    out = []
    for tk, g in daily.groupby("Ticker", as_index=False):
        g = g.sort_values("Date").set_index("Date")
        g["pos_ratio_7d"]     = g["pos_ratio_1d"].rolling("7D").mean()
        g["sent_mean_3d_ewm"] = g["sent_mean_1d"].ewm(span=3, adjust=False).mean()
        g["sent_mean_7d"]     = g["sent_mean_1d"].rolling("7D").mean()
        g["sent_surprise_1d"] = g["sent_mean_1d"] - g["sent_mean_7d"]
        g = g.drop(columns=["sent_mean_7d"])
        g["Ticker"] = tk
        out.append(g.reset_index())
    daily_T = pd.concat(out, ignore_index=True)
    daily_T["Date"] = pd.to_datetime(daily_T["Date"])
    keep = ["Ticker","Date","sent_mean_1d","pos_ratio_7d","news_count_1d","sent_mean_3d_ewm","sent_surprise_1d"]
    return daily_T[keep]

def aggregate_daily_tplus1_sparse(df_scored: pd.DataFrame) -> pd.DataFrame:
    if df_scored is None or df_scored.empty:
        return pd.DataFrame(columns=["Ticker","Date","sent_mean_1d","pos_ratio_7d","news_count_1d","sent_mean_3d_ewm","sent_surprise_1d"])
    df = df_scored.copy()
    df["Date"] = pd.to_datetime(df["publish_dt"], errors="coerce").dt.tz_localize(None).dt.normalize()
    df = df.dropna(subset=["Date"])
    daily_T = (df.groupby(["Ticker","Date"], as_index=False)
                 .agg(sent_mean_1d=("sent_score","mean"),
                      news_count_1d=("sent_score","size"),
                      pos_ratio_1d=("sent_score", lambda x: float(np.mean(np.array(x) > 0)))))
    daily_T["Date"] = pd.to_datetime(daily_T["Date"]) + BDay(1)  # ç”Ÿæ•ˆæ—¥=ä¸‹ä¸€å·¥ä½œæ—¥
    out = []
    for tk, g in daily_T.groupby("Ticker", as_index=False):
        g = g.sort_values("Date").set_index("Date")
        g["pos_ratio_7d"]     = g["pos_ratio_1d"].rolling("7D").mean()
        g["sent_mean_3d_ewm"] = g["sent_mean_1d"].ewm(span=3, adjust=False).mean()
        g["sent_mean_7d"]     = g["sent_mean_1d"].rolling("7D").mean()
        g["sent_surprise_1d"] = g["sent_mean_1d"] - g["sent_mean_7d"]
        g = g.drop(columns=["sent_mean_7d"])
        g["Ticker"] = tk
        out.append(g.reset_index())
    daily_Tp1 = pd.concat(out, ignore_index=True)
    daily_Tp1["Date"] = pd.to_datetime(daily_Tp1["Date"])
    keep = ["Ticker","Date","sent_mean_1d","pos_ratio_7d","news_count_1d","sent_mean_3d_ewm","sent_surprise_1d"]
    return daily_Tp1[keep]

# ---------- 7) å…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰ ----------
STOPWORDS = set("""
a an the and or for to in of on at from by with as is are was were be been being
this that these those it its their them they he she you your i me my we us our
und der die das den dem des ein eine einem einer eines mit ohne aber oder nicht
""".split())
def extract_keywords(title: str, body: str, topk: int = 8) -> str:
    text = f"{title or ''} {body or ''}".lower()
    toks = re.findall(r"[a-z0-9]+", text)
    toks = [t for t in toks if t not in STOPWORDS and len(t) >= 3]
    if not toks: return ""
    vc = pd.Series(toks).value_counts()
    return "ã€".join(vc.head(topk).index.tolist())

# ---------- 8) ä¸»å‡½æ•°ï¼šä¸€é”®è·‘ & ä¿å­˜ ----------
def run_sentiment_and_save(
    tickers,
    api_key: str = None,
    drive_dir: str = SAVE_DIR,
    save_prefix: str = SAVE_PREFIX,
    dedup_thresh: float = 0.80,
):
    key = api_key or os.getenv("SERPAPI_API_KEY") or (globals().get("SERP_API_KEY") or "")
    if not key:
        raise RuntimeError("Missing SerpAPI key: pass api_key or set SERPAPI_API_KEY / SERP_API_KEY.")
    os.makedirs(drive_dir, exist_ok=True)
    today_str = datetime.now(ZoneInfo("Europe/Berlin")).date().isoformat()

    # 1) æŠ“
    df_articles = discover_and_expand_articles(tickers, key)
    if df_articles.empty:
        print("âš ï¸ discover è¿”å›ç©º")
        empty_daily = pd.DataFrame(columns=["Ticker","Date","sent_mean_1d","pos_ratio_7d","news_count_1d","sent_mean_3d_ewm","sent_surprise_1d"])
        empty_news  = pd.DataFrame(columns=["æ—¥æœŸ","å‘å¸ƒæ—¶é—´","title","æ–°é—»å†…å®¹","æ¥æº","ç½‘å€","é‡ç‚¹è¯æ±‡","ticker","æƒ…ç»ªåˆ†æ•°"])
        empty_daily.to_parquet(f"{drive_dir}/{save_prefix}_daily_T_display_{today_str}.parquet", index=False)
        empty_daily.to_parquet(f"{drive_dir}/{save_prefix}_daily_Tplus1_filtered_{today_str}.parquet", index=False)
        empty_news.to_parquet(f"{drive_dir}/{save_prefix}_news_scored_{today_str}.parquet", index=False)
        return empty_daily, empty_news, empty_daily

    print(f"ğŸ“° discover: æ€»æ¡æ•°={len(df_articles)}; by ticker=", df_articles["Ticker"].value_counts().to_dict())

    # 2) æ‰“åˆ†
    df_scored = batch_score(df_articles, w_title=0.3, w_body=0.7, use_lm=True, lm_weight=0.3)
    print("âœ… after score:", len(df_scored))

    # 3) å»é‡
    out, per_group = [], []
    for tk, g in df_scored.groupby("Ticker"):
        gg = dedup_by_title(g, thresh=dedup_thresh)
        per_group.append((tk, len(g), 0 if gg is None else len(gg)))
        if gg is not None and not gg.empty: out.append(gg)
    df_scored2 = pd.concat(out, ignore_index=True) if out else df_scored.copy()
    print("ğŸ” dedup per ticker (before -> after):", per_group[:10], "...")
    print("âœ… after dedup:", len(df_scored2))

    # 4) é€æ¡æ–°é—»ï¼ˆå¸¦æƒ…ç»ªåˆ†æ•°ï¼‰
    publish_s = pd.to_datetime(df_scored2["publish_dt"], errors="coerce")
    src_series = df_scored2.get("source","")
    if (src_series.fillna("") == "").all():
        src_series = df_scored2.get("url","").fillna("").map(lambda u: urlparse(u).netloc if isinstance(u,str) else "")
    news_scored = pd.DataFrame({
        "æ—¥æœŸ": publish_s.dt.tz_localize(None).dt.date,
        "å‘å¸ƒæ—¶é—´": publish_s.dt.tz_convert(None) if hasattr(publish_s.dt, "tz_convert") else publish_s.dt.tz_localize(None),
        "title": df_scored2.get("title_text",""),
        "æ–°é—»å†…å®¹": df_scored2.get("body_text",""),
        "æ¥æº": src_series,
        "ç½‘å€": df_scored2.get("url",""),
        "é‡ç‚¹è¯æ±‡": [extract_keywords(t, b, topk=8) for t, b in zip(df_scored2.get("title_text",""), df_scored2.get("body_text",""))],
        "ticker": df_scored2["Ticker"],
        "æƒ…ç»ªåˆ†æ•°": df_scored2["sent_score"],
    })
    news_path = f"{drive_dir}/{save_prefix}_news_scored_{today_str}.parquet"
    news_scored.to_parquet(news_path, index=False)
    print("âœ… saved news_scored:", news_path, "; rows=", len(news_scored))

    # 5) ç¨€ç–èšåˆï¼šTï¼ˆå½“å¤©ï¼Œå±•ç¤ºç”¨ï¼‰
    daily_T = aggregate_daily_T_sparse(df_scored2)
    daily_T_path = f"{drive_dir}/{save_prefix}_daily_T_display_{today_str}.parquet"
    daily_T.to_parquet(daily_T_path, index=False)
    print("âœ… saved daily_T (display):", daily_T_path, "; rows=", len(daily_T))

    # 6) ç¨€ç–èšåˆï¼šT+1ï¼ˆç”Ÿæ•ˆæ—¥ï¼Œå»ºæ¨¡ç”¨ï¼‰
    daily_Tplus1 = aggregate_daily_tplus1_sparse(df_scored2)
    daily_Tplus1_filtered = daily_Tplus1[daily_Tplus1["sent_mean_1d"].notna()].copy()
    daily_Tp1_path = f"{drive_dir}/{save_prefix}_daily_Tplus1_filtered_{today_str}.parquet"
    daily_Tplus1_filtered.to_parquet(daily_Tp1_path, index=False)
    print("âœ… saved daily_Tplus1_filtered (model):", daily_Tp1_path, "; rows=", len(daily_Tplus1_filtered))

    return daily_Tplus1_filtered, news_scored, daily_T

# ================== ä½¿ç”¨ç¤ºä¾‹ï¼ˆå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰ ==================
# daily_Tplus1_filtered, news_scored, daily_T = run_sentiment_and_save(
#     ["AAPL","MSFT","AMZN","GOOGL","META","TSLA","NVDA","NFLX","AMD","AVGO"],
#     api_key="<<ä½ çš„SerpAPI Key>>"
# )

# ================== ä½¿ç”¨ç¤ºä¾‹ï¼ˆè¿è¡Œå®Œæˆåå–æ¶ˆæ³¨é‡Šå†æ‰§è¡Œï¼‰ ==================
daily_Tplus1_filtered, news_scored, daily_T = run_sentiment_and_save(
    ["AAPL"], api_key="6c93b9f6f67545ce17c42da1b8969750dbcf0e2d4eed17bb50178e55b8e531f9")
print(daily_Tplus1_filtered.head(), "\n", news_scored.head(), "\n", daily_T.head())

daily_Tplus1_filtered

news_scored.head(3)

daily_T.head(3)

import numpy as np
import pandas as pd

def merge_same_effective_day(daily_Tplus1_filtered: pd.DataFrame) -> pd.DataFrame:
    df = daily_Tplus1_filtered.copy()
    df["Date"] = pd.to_datetime(df["Date"])

    # 1) å…ˆæŒ‰ (Ticker, Date) åˆå¹¶ï¼ˆæƒé‡= news_count_1dï¼‰
    def _merge_grp(g):
        w = g["news_count_1d"].fillna(0).to_numpy(dtype=float)
        # é˜²å…¨0ï¼šç»™æå°æƒé‡é¿å…é™¤é›¶
        w = np.where(w > 0, w, 1.0)
        wtot = np.nansum(w)

        sent = np.nansum(w * g["sent_mean_1d"].to_numpy(dtype=float)) / wtot if wtot > 0 else np.nan
        pos7 = np.nansum(w * g["pos_ratio_7d"].to_numpy(dtype=float)) / wtot if "pos_ratio_7d" in g else np.nan
        news = g["news_count_1d"].sum()

        return pd.Series({
            "sent_mean_1d": sent,
            "pos_ratio_7d": pos7,
            "news_count_1d": news,
        })

    merged = (df.groupby(["Ticker","Date"], as_index=False)
                .apply(_merge_grp)
                .reset_index(drop=True))

    # 2) åŸºäºåˆå¹¶åçš„ sent_mean_1d é‡æ–°è®¡ç®—æ—¶åºæ´¾ç”Ÿç‰¹å¾
    out = []
    for tk, g in merged.groupby("Ticker", as_index=False):
        g = g.sort_values("Date").set_index("Date")

        # EWM(3) ä¸ 7Dçª—å£æƒŠå–œé¡¹ï¼ˆç¨€ç–æ—¶é—´åºåˆ—ï¼ŒæŒ‰è‡ªç„¶æ—¶é—´çª—è®¡ç®—ï¼‰
        g["sent_mean_3d_ewm"] = g["sent_mean_1d"].ewm(span=3, adjust=False).mean()
        g["sent_mean_7d"]     = g["sent_mean_1d"].rolling("7D").mean()
        g["sent_surprise_1d"] = g["sent_mean_1d"] - g["sent_mean_7d"]
        g = g.drop(columns=["sent_mean_7d"])

        g["Ticker"] = tk
        out.append(g.reset_index())

    fixed = pd.concat(out, ignore_index=True)
    fixed = fixed[["Ticker","Date","sent_mean_1d","pos_ratio_7d","news_count_1d","sent_mean_3d_ewm","sent_surprise_1d"]]
    fixed = fixed.sort_values(["Ticker","Date"]).reset_index(drop=True)
    return fixed

# ç”¨æ³•ï¼ˆç›´æ¥å¥—åœ¨ä½ å·²æœ‰ç»“æœä¸Šï¼‰ï¼š
# å‡è®¾ä½ å·²æœ‰ daily_Tplus1_filtered å˜é‡
daily_Tplus1_fixed = merge_same_effective_day(daily_Tplus1_filtered)

# å¦‚æœä½ æƒ³è¦†ç›–ä¿å­˜æˆ–å¦å­˜ä¸ºæ–°æ–‡ä»¶ï¼š
from zoneinfo import ZoneInfo
today_str = pd.Timestamp.now(tz=ZoneInfo("Europe/Berlin")).date().isoformat()
save_path = f"/content/drive/MyDrive/sentiment_daily_Tplus1_filtered_{today_str}_merged.parquet"
daily_Tplus1_fixed.to_parquet(save_path, index=False)
print("âœ… merged & saved:", save_path, "; rows=", len(daily_Tplus1_fixed))

daily_Tplus1_fixed
# -*- coding: utf-8 -*-
"""final_PBSS_QuantModel_complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12or8E-yHAe_6FrdCebVV5iZ9_Dx4lBSa
"""

#挂载
# === 挂载 Google Drive ===
from google.colab import drive
drive.mount('/content/drive')

import os
SAVE_DIR = "/content/drive/MyDrive/models"
os.makedirs(SAVE_DIR, exist_ok=True)
print("✅ 模型将保存到:", SAVE_DIR)

# ==== Parameters ====
TICKER = "AAPL"
START  = "2018-01-01"
LOOKBACK = 120
TEST_RATIO = 0.2
PEER_TICKERS = ["MSFT","AMZN","GOOGL","META","TSLA","NVDA","NFLX","AMD","AVGO"]

# ==== Save/Load config ====
from pathlib import Path
import os, json

SAVE_DIR = "/content/drive/MyDrive/models"
os.makedirs(SAVE_DIR, exist_ok=True)

MODEL_TAG = f"{TICKER}_{START}_lb{LOOKBACK}".replace(":", "-")
XGB_PATH  = os.path.join(SAVE_DIR, f"xgb_{MODEL_TAG}.json")
TFT_PATH  = os.path.join(SAVE_DIR, f"tft_{MODEL_TAG}")
META_PATH = os.path.join(SAVE_DIR, f"meta_{MODEL_TAG}.json")

# === Install dependencies (Colab) ===
# 如遇 torch 相关提示，重跑本单元即可。想用 GPU：Runtime → Change runtime type → T4 GPU。
!pip -q install numpy pandas yfinance xgboost shap ta scikit-learn matplotlib
!pip -q install 'u8darts[torch]'

import os, warnings
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

# Reproducibility
SEED = 42
np.random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

import yfinance as yf
print("Env ready.")

def _safe_pct_change(s, periods=1):
    return s.pct_change(periods=periods).replace([np.inf, -np.inf], np.nan)

def _merge_on_index(left: pd.DataFrame, right: pd.DataFrame):
    return left.join(right, how="left")

def mape_safe(y_true, y_pred):
    y_true = np.asarray(y_true).reshape(-1)
    y_pred = np.asarray(y_pred).reshape(-1)
    n = min(len(y_true), len(y_pred))
    y_true, y_pred = y_true[:n], y_pred[:n]
    denom = np.where(y_true == 0, np.nan, np.abs(y_true))
    return float(np.nanmean(np.abs((y_true - y_pred) / denom)) * 100)

# 价量指标
def add_price_volume_indicators(df: pd.DataFrame) -> pd.DataFrame:
    from ta.momentum import RSIIndicator
    from ta.trend import EMAIndicator, SMAIndicator, MACD
    from ta.volatility import BollingerBands, AverageTrueRange
    from ta.volume import OnBalanceVolumeIndicator

    out = df.copy()
    out["ret_1d"]  = _safe_pct_change(out["Close"], 1)
    out["logret"]  = np.log(out["Close"]).diff()

    for w in [5, 10, 20, 60, 120]:
        out[f"sma_{w}"] = SMAIndicator(out["Close"], window=w).sma_indicator()
        out[f"ema_{w}"] = EMAIndicator(out["Close"], window=w).ema_indicator()

    out["ma_cross_5_20"] = out["sma_5"] - out["sma_20"]
    out["ema_cross_12_26"] = EMAIndicator(out["Close"], 12).ema_indicator() - EMAIndicator(out["Close"], 26).ema_indicator()

    out["rsi_14"] = RSIIndicator(out["Close"], window=14).rsi()

    macd = MACD(out["Close"], window_slow=26, window_fast=12, window_sign=9)
    out["macd"] = macd.macd()
    out["macd_signal"] = macd.macd_signal()
    out["macd_hist"] = macd.macd_diff()

    bb = BollingerBands(out["Close"], window=20, window_dev=2)
    out["bb_high"] = bb.bollinger_hband()
    out["bb_low"]  = bb.bollinger_lband()
    out["bb_mid"]  = bb.bollinger_mavg()
    out["bb_bw"]   = (out["bb_high"] - out["bb_low"]) / out["bb_mid"]
    out["bb_pos"]  = (out["Close"] - out["bb_low"]) / (out["bb_high"] - out["bb_low"])

    out["atr_14"] = AverageTrueRange(out["High"], out["Low"], out["Close"], window=14).average_true_range()
    out["obv"] = OnBalanceVolumeIndicator(out["Close"], out["Volume"]).on_balance_volume()
    out["vol_chg_1d"] = _safe_pct_change(out["Volume"], 1)
    return out

# 滞后&滚动
def add_lags_and_rolling(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for lag in [1, 2, 3, 5, 10]:
        out[f"ret_lag_{lag}"]   = out["ret_1d"].shift(lag)
        out[f"close_lag_{lag}"] = out["Close"].shift(lag)
    for w in [5, 10, 20]:
        out[f"ret_roll_mean_{w}"] = out["ret_1d"].rolling(w).mean()
        out[f"ret_roll_std_{w}"]  = out["ret_1d"].rolling(w).std()
        out[f"vol_annual_{w}"]    = out[f"ret_roll_std_{w}"] * np.sqrt(252)
    for w in [5, 20, 60]:
        out[f"mom_{w}"] = out["Close"] / out["Close"].shift(w) - 1
    return out

def download_macro_panel(start="2015-01-01", end=None) -> pd.DataFrame:
    tickers = {"SPY": "SPY", "VIX": "^VIX", "TNX": "^TNX", "WTI": "CL=F", "GOLD": "GC=F", "EURUSD": "EURUSD=X"}
    data = yf.download(list(tickers.values()), start=start, end=end, auto_adjust=True, group_by="ticker", progress=False)
    frames = []
    for name, ysym in tickers.items():
        try:
            if isinstance(data.columns, pd.MultiIndex):
                s = data[(ysym, "Close")].rename(name)
            else:
                s = data["Close"].rename(name)
            frames.append(s)
        except Exception:
            pass
    if not frames:
        return pd.DataFrame()
    panel = pd.concat(frames, axis=1).sort_index().ffill()
    if "TNX" in panel.columns:
        panel["TNX"] = panel["TNX"] / 100.0
    macro_ret = panel.pct_change().add_suffix("_ret")
    macro = _merge_on_index(panel, macro_ret)
    # 防展望偏差：统一 +1 个交易日
    macro = macro.shift(1)
    return macro

def download_equity_panel(tickers: List[str], start="2015-01-01", end=None) -> pd.DataFrame:
    data = yf.download(tickers, start=start, end=end, auto_adjust=True, group_by="ticker", progress=False)
    frames = []
    for t in tickers:
        try:
            if isinstance(data.columns, pd.MultiIndex):
                s = data[(t, "Close")].rename(f"{t}_Close")
            else:
                s = data["Close"].rename(f"{t}_Close")
            frames.append(s)
        except Exception:
            pass
    if not frames:
        return pd.DataFrame()
    panel = pd.concat(frames, axis=1).sort_index().ffill()
    # 防展望偏差：统一 +1 个交易日
    panel = panel.shift(1)

    ret = panel.pct_change().add_suffix("_ret")
    mom_20 = (panel / panel.shift(20) - 1).add_suffix("_mom20")
    mom_60 = (panel / panel.shift(60) - 1).add_suffix("_mom60")
    peers = pd.concat([ret, mom_20, mom_60], axis=1)
    return peers

def build_feature_matrix(df_raw: pd.DataFrame, macro_df: pd.DataFrame = None, predict_horizon: int = 1, drop_na: bool = True):
    feats = add_price_volume_indicators(df_raw)
    feats = add_lags_and_rolling(feats)
    if macro_df is not None and not macro_df.empty:
        feats = _merge_on_index(feats, macro_df)
    feats["target"] = feats["Close"].shift(-predict_horizon)  # next-day price
    if drop_na:
        feats = feats.dropna()
    X = feats.drop(columns=["target"])
    y = feats["target"].copy()
    return X, y, feats

def select_covariate_columns(feats_train: pd.DataFrame, feats_test: pd.DataFrame):
    common = feats_train.columns.intersection(feats_test.columns)
    common = [c for c in common if pd.api.types.is_numeric_dtype(feats_train[c])]
    cols_ok = []
    for c in common:
        t_ok = feats_train[c].notna().any()
        v_ok = feats_test[c].notna().any()
        not_const = (feats_train[c].std(skipna=True) > 0) or (feats_test[c].std(skipna=True) > 0)
        if t_ok and v_ok and not_const:
            cols_ok.append(c)
    if "Close" in feats_train.columns and "Close" not in cols_ok:
        cols_ok.append("Close")
    return cols_ok

def shap_select_features(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame,
                         top_k: int=None, cum_importance: float=0.9, seed: int=42):
    import shap
    from xgboost import XGBRegressor
    model = XGBRegressor(
        n_estimators=2000, max_depth=6, learning_rate=0.03,
        subsample=0.9, colsample_bytree=0.9, random_state=seed, n_jobs=-1,
        reg_lambda=1.0
    )
    model.fit(X_train, y_train, verbose=False)
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_valid)
    imp = np.abs(shap_values).mean(axis=0)
    imp = pd.Series(imp, index=X_valid.columns).sort_values(ascending=False)
    imp_norm = imp / max(imp.sum(), 1e-12)

    if top_k is None:
        cum = imp_norm.cumsum()
        selected = list(cum[cum <= cum_importance].index)
        if len(selected) < 20:
            selected = list(imp_norm.index[:20])
    else:
        selected = list(imp_norm.index[:top_k])
    return selected, imp_norm

from pathlib import Path

def run_tft_direction_with_feats(
    feats_train, feats_test,
    lookback=120, n_epochs=120, dropout=0.05, seed=42,
    tft_save_path=None, return_model=False
):
    import torch, numpy as np, pandas as pd
    from darts import TimeSeries
    from darts.models import TFTModel
    from darts.dataprocessing.transformers import Scaler as DartsScaler
    from darts.utils.likelihood_models import BernoulliLikelihood
    from sklearn.metrics import f1_score

    np.random.seed(seed); torch.manual_seed(seed)

    # --- 时间索引对齐 ---
    for df_ in (feats_train, feats_test):
        df_.index = pd.to_datetime(df_.index).tz_localize(None)

    df_all = pd.concat([feats_train, feats_test]).sort_index()
    full_idx = pd.date_range(df_all.index.min(), df_all.index.max(), freq="B")
    df_all = df_all.reindex(full_idx).ffill()

    if "Close" not in df_all.columns:
        raise ValueError("Features must contain 'Close' column.")

    # 下日方向标签
    cls_all = (df_all["Close"].shift(-1) > df_all["Close"]).astype(float)
    df_all, cls_all = df_all.iloc[:-1, :], cls_all.iloc[:-1]

    # 按 feats_train 的末日切分
    split_point = pd.to_datetime(feats_train.index[-1]).tz_localize(None)
    cut_idx = min(np.searchsorted(full_idx, split_point) + 1, len(df_all.index))
    if cut_idx >= len(df_all.index):
        from pandas import DatetimeIndex
        return np.array([]), np.array([]), float("nan"), DatetimeIndex([]), 0.5 if not return_model else (np.array([]), np.array([]), float("nan"), DatetimeIndex([]), 0.5, None)

    train_slice = df_all.iloc[:cut_idx]
    valid_slice = df_all.iloc[cut_idx:]

    # 你自己的列筛选函数（假设已定义）
    cols_ok = select_covariate_columns(train_slice, valid_slice)
    if "Close" not in cols_ok and "Close" in df_all.columns:
        cols_ok.append("Close")

    # --- 构建 TimeSeries ---
    cls_ts = TimeSeries.from_times_and_values(df_all.index, cls_all.values).astype(np.float32)

    df_all_reset = df_all.reset_index()
    df_all_reset.columns = ["Date"] + list(df_all.columns)
    past_cov = TimeSeries.from_dataframe(
        df_all_reset[["Date"] + cols_ok],
        time_col="Date", value_cols=cols_ok, fill_missing_dates=True, freq="B"
    ).astype(np.float32)

    cls_tr, cls_te   = cls_ts[:cut_idx], cls_ts[cut_idx:]
    past_tr, past_te = past_cov[:cut_idx], past_cov[cut_idx:]

    common = sorted(list(set(past_tr.components) & set(past_te.components)))
    past_tr = past_tr[common]; past_te = past_te[common]

    sc_cov = DartsScaler()
    past_tr_s = sc_cov.fit_transform(past_tr).astype(np.float32)
    past_te_s = sc_cov.transform(past_te).astype(np.float32)
    past_all_s = sc_cov.transform(past_cov).astype(np.float32)
    past_tr_s = past_tr_s[common]; past_te_s = past_te_s[common]; past_all_s = past_all_s[common]

    # --- 建模 & 训练 ---
    tft = TFTModel(
        input_chunk_length=lookback, output_chunk_length=1,
        hidden_size=64, lstm_layers=1, dropout=dropout,
        batch_size=32, n_epochs=n_epochs, random_state=seed,
        add_relative_index=True, likelihood=BernoulliLikelihood(),
        pl_trainer_kwargs={"accelerator":"cpu","devices":1, "enable_checkpointing":False},
        force_reset=True, save_checkpoints=False
    )

    tft.fit(
        series=cls_tr,
        past_covariates=past_tr_s,
        val_series=cls_te,
        val_past_covariates=past_te_s,
        verbose=False
    )

    # --- 预测（先算出概率/方向/指标/阈值）---
    proba_ts = tft.predict(n=len(cls_te), series=cls_tr, past_covariates=past_all_s)

    def _ts_to_series(ts):
        for attr in ("to_series", "pd_series"):
            if hasattr(ts, attr):
                try:
                    return getattr(ts, attr)()
                except Exception:
                    pass
        return ts.to_dataframe().iloc[:, 0]

    proba_pd = _ts_to_series(proba_ts)
    proba_pd.index = pd.to_datetime(proba_pd.index).tz_localize(None)

    val_true = _ts_to_series(cls_te).astype(int)
    idx_common = val_true.index.intersection(proba_pd.index)
    val_true = val_true.reindex(idx_common)
    val_proba = proba_pd.reindex(idx_common).values

    ths = np.linspace(0.3, 0.7, 41)
    f1s = [f1_score(val_true.values, (val_proba >= t).astype(int)) for t in ths] if len(val_true) else [0]
    best_t = float(ths[int(np.argmax(f1s))]) if len(val_true) else 0.5

    pred_up = (val_proba >= best_t).astype(int) if len(val_true) else np.array([])
    DA = float((pred_up == val_true.values).mean()) if len(val_true) else float("nan")

    # --- 保存模型（放在 return 之前；确保目录存在）---
    save_path = (tft_save_path or TFT_PATH)
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    tft.save(save_path)
    print(f"✅ Saved TFT -> {save_path}")

    # --- 返回结果 ---
    if return_model:
        return pred_up, val_proba, DA, idx_common, best_t, tft
    else:
        return pred_up, val_proba, DA, idx_common, best_t

def simple_pnl(signals_df: pd.DataFrame, fee_bps: float=1.0) -> Tuple[Dict, pd.DataFrame]:
    df = signals_df.copy()
    df["ret_1d"] = pd.Series(df["TrueClose"]).pct_change().fillna(0.0)
    pos_map = {"Buy":1, "Sell":-1, "Hold":0}
    df["pos"] = df["Signal"].map(pos_map).fillna(0).astype(int)
    df["pos_prev"] = df["pos"].shift(1).fillna(0).astype(int)
    change = (df["pos"] != df["pos_prev"]).astype(int)
    cost = change * (fee_bps / 1e4)
    strat_ret = df["pos_prev"] * df["ret_1d"] - cost
    df["strat_eq"] = (1.0 + strat_ret).cumprod()
    df["bh_eq"] = (1.0 + df["ret_1d"]).cumprod()
    cagr = df["strat_eq"].iloc[-1] ** (252/len(df)) - 1 if len(df) > 0 else np.nan
    vol  = np.sqrt(252) * strat_ret.std() if len(df) > 1 else np.nan
    sharpe = cagr/vol if (vol and vol>0) else np.nan
    return {
        "STRAT_FinalEq": float(df["strat_eq"].iloc[-1]) if len(df)>0 else np.nan,
        "BH_FinalEq": float(df["bh_eq"].iloc[-1]) if len(df)>0 else np.nan,
        "CAGR": float(cagr) if pd.notna(cagr) else float("nan"),
        "Vol": float(vol) if pd.notna(vol) else float("nan"),
        "Sharpe": float(sharpe) if pd.notna(sharpe) else float("nan"),
        "Turns": int(change.sum())
    }, df[["Date","Signal","strat_eq","bh_eq"]]

# ==== Download target ====
df = yf.download(TICKER, start=START, end=None, auto_adjust=True, progress=False)
if isinstance(df.columns, pd.MultiIndex):
    df = df.loc[:, (slice(None), TICKER)]; df.columns = df.columns.droplevel(1)
df = df[["Open","High","Low","Close","Volume"]].sort_index()
if df.empty:
    raise RuntimeError("Empty data.")

# ==== Macro & peers ====
macro = download_macro_panel(start=df.index.min().strftime("%Y-%m-%d"))
peers = download_equity_panel(PEER_TICKERS, start=df.index.min().strftime("%Y-%m-%d"))
aux_panel = pd.concat([macro, peers], axis=1)

# ==== Feature matrix ====
X_all, y_all, feats_all = build_feature_matrix(df, macro_df=aux_panel, predict_horizon=1)
feats_all["target_ret"] = feats_all["target"] / feats_all["Close"] - 1

# ==== Split ====
split_idx = int(len(feats_all) * (1 - TEST_RATIO))
feats_tr_all = feats_all.iloc[:split_idx]
feats_te_all = feats_all.iloc[split_idx:]

valid_len = max( int(len(feats_tr_all)*0.15), 60 )
feats_tr = feats_tr_all.iloc[:-valid_len]
feats_va = feats_tr_all.iloc[-valid_len:]

def _Xy(df_):
    X = df_.drop(columns=["target","target_ret"]).fillna(method="ffill").fillna(0.0)
    y = df_["target_ret"]
    return X, y

X_tr, y_tr = _Xy(feats_tr)
X_va, y_va = _Xy(feats_va)
X_te = feats_te_all.drop(columns=["target","target_ret"]).fillna(method="ffill").fillna(0.0)
y_te_price = feats_te_all["target"]
price_t_te = feats_te_all["Close"]

# ==== SHAP feature selection ====
selected_cols, shap_imp = shap_select_features(X_tr, y_tr, X_va, top_k=None, cum_importance=0.9, seed=SEED)
print(f"Selected {len(selected_cols)} features")
shap_imp.to_csv("shap_importance.csv")

# ==== XGBoost via DMatrix (version-agnostic, with early stopping) ====
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error

X_tr_s = X_tr[selected_cols]
X_va_s = X_va[selected_cols]
X_te_s = X_te[selected_cols]

dtrain = xgb.DMatrix(pd.concat([X_tr_s, X_va_s]), label=pd.concat([y_tr, y_va]))
dvalid = xgb.DMatrix(X_va_s, label=y_va)
dtest  = xgb.DMatrix(X_te_s)

params = {
    "objective": "reg:squarederror",
    "eval_metric": "rmse",
    "max_depth": 6,
    "eta": 0.02,
    "subsample": 0.9,
    "colsample_bytree": 0.9,
    "seed": SEED,
    "tree_method": "hist",
    # "device": "cuda",   # 如启用 Colab GPU，取消注释
}

booster = xgb.train(
    params,
    dtrain,
    num_boost_round=4000,
    evals=[(dvalid, "valid")],
    early_stopping_rounds=300,
    verbose_eval=False,
)
# === SAVE XGBoost model ===
try:
    booster.save_model(XGB_PATH)
    print(f"✅ Saved XGB -> {XGB_PATH}")
except NameError:
    import os
    os.makedirs("./models", exist_ok=True)
    _xgb_path = "./models/xgb_autosave.json"
    booster.save_model(_xgb_path)
    print(f"✅ Saved XGB (fallback) -> {_xgb_path}")

if hasattr(booster, "best_iteration") and booster.best_iteration is not None:
    y_pred_ret = pd.Series(
        booster.predict(dtest, iteration_range=(0, booster.best_iteration + 1)),
        index=X_te_s.index,
    )
else:
    y_pred_ret = pd.Series(booster.predict(dtest), index=X_te_s.index)

y_pred_price = price_t_te * (1.0 + y_pred_ret)
rmse = float(np.sqrt(mean_squared_error(y_te_price, y_pred_price)))
mae  = float(mean_absolute_error(y_te_price, y_pred_price))
mape = mape_safe(y_te_price, y_pred_price)
print(f"XGB (Test) -> RMSE={rmse:.4f}  MAE={mae:.4f}  MAPE={mape:.2f}%")

# ==== TFT with selected covariates (FIX: always include Close) ====
selected_cols_tft = list(dict.fromkeys(["Close"] + list(selected_cols)))

base_train = feats_tr_all.drop(columns=["target","target_ret"])
base_test  = feats_te_all.drop(columns=["target","target_ret"])
assert "Close" in base_train.columns and "Close" in base_test.columns, "Close col missing in base features."

cols_exist_train = [c for c in selected_cols_tft if c in base_train.columns]
cols_exist_test  = [c for c in selected_cols_tft if c in base_test.columns]
common_tft_cols  = sorted(set(cols_exist_train) & set(cols_exist_test))

feats_train_cov = base_train[common_tft_cols]
feats_test_cov  = base_test[common_tft_cols]
print(f"TFT will use {len(common_tft_cols)} covariates (includes Close: {'Close' in common_tft_cols}).")

pred_up, proba_up, DA, idx_common, best_thresh, tft = run_tft_direction_with_feats(
    feats_train_cov, feats_test_cov,
    lookback=LOOKBACK, n_epochs=120, dropout=0.05,
    tft_save_path=TFT_PATH, return_model=True
)

print(f"TFT (Test) -> DA={DA:.4f}, calibrated threshold={best_thresh:.3f}, test days used: {len(idx_common)}")

# ==== Build signals ====
y_pred_price_c = y_pred_price.reindex(idx_common)
y_true_price_c = y_te_price.reindex(idx_common)
price_t_c      = price_t_te.reindex(idx_common)

half = int(len(idx_common)*0.5)
grid = [0.002, 0.005, 0.008, 0.01, 0.015]

def build_signals(ret_thresh):
    signals = []
    for i, dt in enumerate(idx_common):
        if i == 0:
            signals.append("Hold"); continue
        ret_pred = (y_pred_price_c.iloc[i-1] - price_t_c.iloc[i-1]) / price_t_c.iloc[i-1]
        if pred_up[i-1] == 1 and ret_pred > ret_thresh:
            signals.append("Buy")
        elif pred_up[i-1] == 0 and ret_pred < -ret_thresh:
            signals.append("Sell")
        else:
            signals.append("Hold")
    return signals

best_rt, best_eq = grid[0], -1
for g in grid:
    sigs = build_signals(g)
    df_sig = pd.DataFrame({
        "Date": idx_common[1:],
        "TrueClose": y_true_price_c.iloc[1:].values,
        "PredClose": y_pred_price_c.iloc[1:].values,
        "TFT_dir": pred_up[1:],
        "TFT_proba": proba_up[1:],
        "Signal": sigs[1:]
    }).reset_index(drop=True)
    stats_tmp, curve_tmp = simple_pnl(df_sig.iloc[:max(2, half-1)])
    if stats_tmp["STRAT_FinalEq"] > best_eq:
        best_eq, best_rt = stats_tmp["STRAT_FinalEq"], g

final_signals = build_signals(best_rt)
df_signals = pd.DataFrame({
    "Date": idx_common[1:],
    "TrueClose": y_true_price_c.iloc[1:].values,
    "PredClose": y_pred_price_c.iloc[1:].values,
    "TFT_dir": pred_up[1:],
    "TFT_proba": proba_up[1:],
    "Signal": final_signals[1:]
}).reset_index(drop=True)

stats, curve = simple_pnl(df_signals)

print("\n=== Strategy Backtest (Test) ===")
for k,v in stats.items():
    print(f"{k}: {v:.4f}" if isinstance(v,float) else f"{k}: {v}")
print(f"RET_THRESH (validated): {best_rt:0.3%}")

df_signals.to_csv("signals_test.csv", index=False)
curve.to_csv("equity_curve_test.csv", index=False)
shap_imp.to_csv("shap_importance.csv", index=True)
print("Saved: shap_importance.csv, signals_test.csv, equity_curve_test.csv")

import matplotlib.pyplot as plt

curve = pd.read_csv("equity_curve_test.csv")
plt.figure(figsize=(8,4))
plt.plot(curve["strat_eq"], label="Strategy")
plt.plot(curve["bh_eq"], label="Buy & Hold")
plt.title("Equity Curve (Test)")
plt.legend()
plt.show()

meta = {
    "ticker": str(TICKER),
    "start": str(START),
    "lookback": int(LOOKBACK),
    "selected_cols_xgb": list(map(str, selected_cols)) if 'selected_cols' in globals() else None,
    "selected_cols_tft": list(map(str, selected_cols_tft)) if 'selected_cols_tft' in globals() else None,
    "test_ratio": float(TEST_RATIO) if 'TEST_RATIO' in globals() else None,
}
# 保存到 META_PATH 或 ./models/meta_autosave.json

from pathlib import Path
print("TFT exists:", Path(TFT_PATH).exists())
print("XGB exists:", Path(XGB_PATH).exists())
print("META exists:", Path(META_PATH).exists())